import streamlit as st
import pandas as pd
import torch
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from botorch.models import SingleTaskGP
from gpytorch.mlls import ExactMarginalLogLikelihood
from botorch.fit import fit_gpytorch_mll
from botorch.acquisition.analytic import ExpectedImprovement
from botorch.optim import optimize_acqf

# 1. ì œëª©
st.title("ğŸ”¬ Slurry ì¡°ì„± ì¶”ì²œ (Bayesian Optimization ê¸°ë°˜)")

# 2. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
try:
    df = pd.read_csv("slurry_data.csv")
except FileNotFoundError:
    st.error("âŒ slurry_data.csv íŒŒì¼ì´ í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— ì—†ìŠµë‹ˆë‹¤.")
    st.stop()

# 3. ì…ë ¥(X), ì¶œë ¥(Y) ì„¤ì •
x_cols = ["graphite_g", "carbon_black_g", "CMC_g", "solvent_g"]
y_col = "yield_stress"

X = df[x_cols].values
Y = df[[y_col]].values  # shape: (n, 1)

# 4. MinMax ì •ê·œí™”
x_scaler = MinMaxScaler()
X_scaled = x_scaler.fit_transform(X)

y_tensor = torch.tensor(Y, dtype=torch.double)
x_tensor = torch.tensor(X_scaled, dtype=torch.double)

# 5. ëª¨ë¸ í•™ìŠµ
model = SingleTaskGP(x_tensor, y_tensor)
mll = ExactMarginalLogLikelihood(model.likelihood, model)
fit_gpytorch_mll(mll)

# 6. íšë“ í•¨ìˆ˜ ì •ì˜ (EI ì‚¬ìš©)
best_y = y_tensor.max()
acq_fn = ExpectedImprovement(model=model, best_f=best_y, maximize=True)

# 7. í›„ë³´ ì¡°ì„± ìµœì í™” (ì •ê·œí™” ë²”ìœ„ [0, 1])
bounds = torch.stack([
    torch.zeros(x_tensor.shape[1], dtype=torch.double),
    torch.ones(x_tensor.shape[1], dtype=torch.double)
])

candidate_scaled, _ = optimize_acqf(
    acq_function=acq_fn,
    bounds=bounds,
    q=1,
    num_restarts=5,
    raw_samples=20,
)

# 8. ì—­ì •ê·œí™” í›„ ê²°ê³¼ ì¶œë ¥
candidate_np = candidate_scaled.detach().numpy()
recommended = x_scaler.inverse_transform(candidate_np)

st.subheader("ğŸ“Œ ì¶”ì²œëœ ì¡°ì„± (ì›ë˜ ë‹¨ìœ„)")
for i, name in enumerate(x_cols):
    st.write(f"- {name}: {recommended[0][i]:.4f} g")
