import streamlit as st
import pandas as pd
import torch
from sklearn.preprocessing import MinMaxScaler
from botorch.models import SingleTaskGP
from botorch.fit import fit_gpytorch_mll
from botorch.acquisition.analytic import ExpectedImprovement
from botorch.optim import optimize_acqf
from gpytorch.mlls import ExactMarginalLogLikelihood

st.title("ğŸ”¬ Slurry ì¡°ì„± ì¶”ì²œ (Bayesian Optimization ê¸°ë°˜)")

# 1. ë°ì´í„° ë¡œë”©
df = pd.read_csv("slurry_data.csv")

# 2. ì…ë ¥(X)ê³¼ ì¶œë ¥(y) ì„¤ì •
x_cols = ["graphite_g", "carbon_black_g", "CMC_g", "solvent_g"]
y_cols = ["yield_stress"]  # ì˜ˆ: yield_stressë§Œ ìµœì í™” ëŒ€ìƒ

X_raw = df[x_cols].values
Y_raw = df[y_cols].values

# 3. ì •ê·œí™”
x_scaler = MinMaxScaler()
X_scaled = x_scaler.fit_transform(X_raw)

# 4. Tensorë¡œ ë³€í™˜ (ì£¼ì˜: train_yëŠ” .unsqueeze(-1) + .detach() í•„ìš”)
train_x = torch.tensor(X_scaled, dtype=torch.double)
train_y = torch.tensor(Y_raw, dtype=torch.double).unsqueeze(-1).detach()

# 5. Gaussian Process ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
model = SingleTaskGP(train_x, train_y)
mll = ExactMarginalLogLikelihood(model.likelihood, model)
fit_gpytorch_mll(mll)

# 6. Expected Improvement ê³„ì‚°
best_y = train_y.max()
acq_fn = ExpectedImprovement(model=model, best_f=best_y.item(), maximize=True)

# 7. ìµœì ì˜ ì¡°ì„± íƒìƒ‰
bounds = torch.stack([
    torch.zeros(train_x.shape[1], dtype=torch.double),
    torch.ones(train_x.shape[1], dtype=torch.double)
])

candidate_scaled, _ = optimize_acqf(
    acq_function=acq_fn,
    bounds=bounds,
    q=1,
    num_restarts=5,
    raw_samples=20,
)

# 8. ì¶”ì²œëœ ì¡°ì„± ë³µì›
candidate_np = candidate_scaled.detach().numpy()
candidate_original = x_scaler.inverse_transform(candidate_np)

# 9. ê²°ê³¼ ì¶œë ¥
st.subheader("âœ… ì¶”ì²œëœ ì¡°ì„± (ì›ë˜ ë‹¨ìœ„ ê¸°ì¤€)")
for i, name in enumerate(x_cols):
    st.write(f"**{name}**: {candidate_original[0][i]:.4f} g")
